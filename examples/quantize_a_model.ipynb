{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e826c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers huggingface_hub \n",
    "# !pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77294af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-11-29 13:36:25 - huggingface_hub._login - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Hugging face hub login\n",
    "import huggingface_hub\n",
    "with open(\"../.env\", \"r\") as f:\n",
    "    token = f.read().strip().split('=')[1]\n",
    "huggingface_hub.login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677de909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load llama 3.1 8B model and quantize it with Auto-AdpQ\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from auto_adpq import Auto_AdpQ, AutoAdpQConfig\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3085140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dff38adff147c29df0da18fa4810e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I have it locally stored on my computer but if it's not the case, it will be downloaded from HF\n",
    "save_path = \"../../MasterThesis/experiments/weights/meta-llama/Meta-Llama-3.1-8B-weights\" \n",
    "# Check if the model is present\n",
    "files = glob.glob(os.path.join(save_path, \"*.safetensors\"))\n",
    "if len(files) == 0:\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "else:\n",
    "    model_name = save_path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", torch_dtype=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96f0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Auto-AdpQ configuration\n",
    "adpq_config = AutoAdpQConfig(\n",
    "    group_size = 128,\n",
    "    n_iters = 100,\n",
    "    alpha = 0.08,\n",
    "    device = \"cpu\",\n",
    "    q_bit = 4,\n",
    "    data_packing = True,\n",
    "    symmetrical_quantization = True\n",
    ")\n",
    "\n",
    "adpq = Auto_AdpQ(config=adpq_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0044495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 13:36:31 - auto_adpq.module - INFO - Quantizing layer: model.layers.0.self_attn.q_proj\n",
      "2025-11-29 13:47:33 - auto_adpq.module - INFO - Quantizing layer: model.layers.0.self_attn.k_proj\n",
      "2025-11-29 13:49:59 - auto_adpq.module - INFO - Quantizing layer: model.layers.0.self_attn.v_proj\n",
      "2025-11-29 13:53:08 - auto_adpq.module - INFO - Quantizing layer: model.layers.0.self_attn.o_proj\n",
      "2025-11-29 14:04:26 - auto_adpq.module - INFO - Quantizing layer: model.layers.0.mlp.gate_proj\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quantize the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m quantized_model = \u001b[43madpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the quantized model\u001b[39;00m\n\u001b[32m      5\u001b[39m adpq.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33mquantized_meta_llama_3.1_8B_adpq\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\auto_adpq\\src\\auto_adpq\\module.py:766\u001b[39m, in \u001b[36mquantize_model\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    764\u001b[39m if module.weight.dtype == torch.bfloat16:\n\u001b[32m    765\u001b[39m     weight_array = module.weight.to(torch.float16).detach().cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m else:\n\u001b[32m    767\u001b[39m     weight_array = module.weight.detach().cpu().numpy()\n\u001b[32m    769\u001b[39m logger.info(f\"Quantizing layer: {name}\")\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\auto_adpq\\src\\auto_adpq\\module.py:668\u001b[39m, in \u001b[36mAdpQ_quantize\u001b[39m\u001b[34m(self, matrix)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mAdpQ_quantize\u001b[39m(\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m, matrix: Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], np.ndarray, torch.Tensor]\n\u001b[32m    657\u001b[39m ) -> AdpQQuantizedWeights:\n\u001b[32m    658\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Quantize a matrix using the AdpQ (LASSO-based) flow.\u001b[39;00m\n\u001b[32m    659\u001b[39m \n\u001b[32m    660\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[33;03m        matrix (Union[list, np.ndarray, torch.Tensor]): Input weight\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[33;03m            matrix. The method reshapes the input to ``(-1, group_size)``\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[33;03m            and processes each group independently.\u001b[39;00m\n\u001b[32m    664\u001b[39m \n\u001b[32m    665\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[33;03m        AdpQQuantizedWeights: Container with quantized values, scales,\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[33;03m            optional zeropoints and outlier indices.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    669\u001b[39m     original_shape = matrix.shape\n\u001b[32m    670\u001b[39m     matrix = matrix.reshape((-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.group_size))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\auto_adpq\\src\\auto_adpq\\module.py:576\u001b[39m, in \u001b[36mlasso_outlier_detection\u001b[39m\u001b[34m(self, matrix)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\auto_adpq\\src\\auto_adpq\\module.py:489\u001b[39m, in \u001b[36m_optimization_function\u001b[39m\u001b[34m(self, matrix, lambda_prime)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Quantize the model\n",
    "quantized_model = adpq.quantize_model(model)\n",
    "\n",
    "# Save the quantized model\n",
    "adpq.save_pretrained(\"quantized_meta_llama_3.1_8B_adpq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5beea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
